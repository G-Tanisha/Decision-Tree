{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Decision Tree Assignment\n"
      ],
      "metadata": {
        "id": "njpKm38lDK-h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  What is a Decision Tree, and how does it work in the context of\n",
        "classification?\n",
        "    - A Decision Tree is a supervised machine learning algorithm commonly used for classification tasks. It works by breaking down a dataset into smaller and smaller subsets through a series of decisions, forming a tree-like structure. Each internal node of the tree represents a test on a feature, each branch represents the outcome of that test, and each leaf node corresponds to a class label or prediction. The algorithm selects the best feature to split the data at each step using measures such as Information Gain or Gini Impurity, aiming to create the most distinct separation between classes. This process continues recursively until the data in a node belong to the same class or a stopping condition like maximum depth is reached. In classification, the final prediction is made by following the path of conditions down the tree until reaching a leaf node, which assigns the class label. Decision Trees are easy to interpret and visualize, but they can overfit the training data if not properly pruned or regularized.\n",
        "\n",
        "2. Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "How do they impact the splits in a Decision Tree?\n",
        "  - n Decision Trees, impurity measures are used to decide how to split the data at each node so that the resulting subsets are as pure (homogeneous) as possible. Two of the most common impurity measures are Gini Impurity and Entropy. Gini Impurity measures how often a randomly chosen sample from a node would be incorrectly classified if it were randomly labeled according to the class distribution in that node. A Gini value of 0 means the node is pure (all samples belong to one class), while higher values indicate more impurity\n",
        "  Entropy, on the other hand, is based on the concept of information theory. It measures the amount of uncertainty (or disorder) in a node. Entropy is 0 when the node is pure and reaches its maximum when the classes are equally mixed.When building a Decision Tree, these measures impact how the splits are chosen. The algorithm evaluates all possible features and thresholds, then selects the split that results in the greatest reduction in impurity—known as Information Gain when using entropy, or simply the decrease in Gini Impurity when using Gini. In essence, both measures aim to make the resulting child nodes as pure as possible, which improves the tree’s ability to classify new data accurately.\n",
        "\n",
        "3. What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "Trees? Give one practical advantage of using each.\n",
        "   - Pre-pruning and Post-pruning are two techniques used to prevent Decision Trees from overfitting.\n",
        "\n",
        "Pre-pruning (Early Stopping): In pre-pruning, the tree-building process is stopped early, before it grows into a fully complex tree. This is done by setting conditions such as maximum depth, minimum number of samples per node, or minimum information gain required to split. The idea is to stop splitting when further divisions are unlikely to provide significant improvement.\n",
        "\n",
        "Practical Advantage: It reduces training time and computational cost since the tree is kept shallow and simple.\n",
        "\n",
        "Post-pruning (Pruning after Full Growth): In post-pruning, the tree is first allowed to grow fully, possibly leading to overfitting, and then it is pruned back by removing branches that do not contribute significantly to accuracy. This is often done using validation data or cost-complexity pruning to balance accuracy and simplicity.\n",
        "\n",
        "Practical Advantage: It generally results in better accuracy on unseen data, since the tree first learns all possible patterns and then removes only the unnecessary complexity.\n",
        "\n",
        "4. What is Information Gain in Decision Trees, and why is it important for\n",
        "choosing the best split?\n",
        "   - Information Gain (IG) is a metric used in Decision Trees to decide which feature and threshold should be chosen for splitting the dataset at a node. It is based on the concept of entropy from information theory, which measures the impurity or uncertainty in a dataset. Information Gain measures how much “uncertainty” is reduced by splitting on a particular feature. A higher IG means the split produces more homogeneous subsets (purer nodes), which improves the decision-making ability of the tree. Information Gain is crucial because it helps the algorithm pick the best feature at each step of tree construction. Without it (or similar measures like Gini Index), the tree would not know which splits make the data more organized by class. Choosing splits with the highest IG ensures the tree becomes more efficient and accurate at classification, while minimizing unnecessary complexity.\n",
        "\n",
        "5. What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "   - Decision Trees are widely used in real-world applications such as medical diagnosis in healthcare, credit scoring and fraud detection in finance, customer segmentation and churn prediction in marketing, and quality control in manufacturing. Their main advantages are that they are simple to interpret, can handle both numerical and categorical data without feature scaling, and work well even with smaller datasets. However, they also have limitations, such as a tendency to overfit if not pruned, instability with small changes in data, bias toward features with many categories, and generally lower accuracy compared to ensemble methods like Random Forests.\n"
      ],
      "metadata": {
        "id": "d6BJKCTlDwzT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''6. Dataset Info:\n",
        "● Iris Dataset for classification tasks (sklearn.datasets.load_iris() or\n",
        "provided CSV).\n",
        "● Boston Housing Dataset for regression tasks\n",
        "(sklearn.datasets.load_boston() or provided CSV).\n",
        "Question 6: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier using the Gini criterion\n",
        "● Print the model’s accuracy and feature importances'''\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "print(\"Model Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLIMEbc6MjqZ",
        "outputId": "55996027-738b-4c47-d2c8-b162f03867d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0191\n",
            "petal length (cm): 0.8933\n",
            "petal width (cm): 0.0876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''7. Question 7: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree.'''\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "clf_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf_limited.fit(X_train, y_train)\n",
        "y_pred_limited = clf_limited.predict(X_test)\n",
        "acc_limited = accuracy_score(y_test, y_pred_limited)\n",
        "\n",
        "clf_full = DecisionTreeClassifier(random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "acc_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "print(\"Accuracy with max_depth=3:\", acc_limited)\n",
        "print(\"Accuracy with fully-grown tree:\", acc_full)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_827fdQSNHoE",
        "outputId": "0546ad19-0712-4b64-89e0-969dc43adaf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with max_depth=3: 1.0\n",
            "Accuracy with fully-grown tree: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 8: Write a Python program to:\n",
        "● Load the Boston Housing Dataset\n",
        "● Train a Decision Tree Regressor\n",
        "● Print the Mean Squared Error (MSE) and feature importances'''\n",
        "\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "boston = fetch_openml(name=\"boston\", version=1, as_frame=True)\n",
        "X, y = boston.data, boston.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "y_pred = regressor.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(X.columns, regressor.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eSCrM1GNlWI",
        "outputId": "a83f8e5f-bdf4-4bd4-c2aa-8e48c0c08fba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 11.588026315789474\n",
            "Feature Importances:\n",
            "CRIM: 0.0585\n",
            "ZN: 0.0010\n",
            "INDUS: 0.0099\n",
            "CHAS: 0.0003\n",
            "NOX: 0.0071\n",
            "RM: 0.5758\n",
            "AGE: 0.0072\n",
            "DIS: 0.1096\n",
            "RAD: 0.0016\n",
            "TAX: 0.0022\n",
            "PTRATIO: 0.0250\n",
            "B: 0.0119\n",
            "LSTAT: 0.1900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 9: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "● Print the best parameters and the resulting model accuracy'''\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Accuracy with Best Model:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLmEB_jdOK-V",
        "outputId": "a1a74525-97ae-4d03-e7b6-a382032d9c34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 10}\n",
            "Accuracy with Best Model: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Question 10: Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "● Handle the missing values\n",
        "● Encode the categorical features\n",
        "● Train a Decision Tree model\n",
        "● Tune its hyperparameters\n",
        "● Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.\n",
        "   - 1. Handle Missing Values: Identify missing data and impute—use mean/median for numerical features and mode or “Unknown” for categorical features.\n",
        "   \n",
        "2. Encode Categorical Features: Apply label encoding for ordinal variables and one-hot encoding for nominal variables.\n",
        "\n",
        "3. Train Decision Tree: Split data into training/testing sets, initialize a Decision Tree classifier, and fit it to the training data.\n",
        "\n",
        "4. Hyperparameter Tuning: Use GridSearchCV or RandomizedSearchCV to optimize max_depth, min_samples_split, min_samples_leaf, and criterion.\n",
        "\n",
        "5. Evaluate Performance: Assess accuracy, precision, recall, F1-score, and ROC-AUC; analyze the confusion matrix and feature importances.\n",
        "\n",
        "6. Business Value: Enables early disease detection, guides clinicians with interpretable rules, optimizes resource allocation, and reduces unnecessary tests, improving patient outcomes and operational efficiency."
      ],
      "metadata": {
        "id": "WZPxhvi-S8uU"
      }
    }
  ]
}